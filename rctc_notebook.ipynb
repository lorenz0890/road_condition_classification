{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.data_access.dao.sussex_huawei_dao import SussexHuaweiDAO\n",
    "from pipeline.feature_engineering.preprocessing.sussex_huawei_preprocessor import SussexHuaweiPreprocessor\n",
    "from pipeline.feature_engineering.feature_extraction.baseline_extractor import BaselineExtractor\n",
    "from pipeline.machine_learning.model.sklearn_model_factory import SklearnModelFactory\n",
    "from pipeline.machine_learning.model.tslearn_model_factory import TslearnModelFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Initialize Pipeline Objects (TODO: Put into a pipleine Facade)\n",
    "dao = SussexHuaweiDAO()\n",
    "preprocessor = SussexHuaweiPreprocessor()\n",
    "extractor = BaselineExtractor()\n",
    "tslearn_factory, sklearn_factory = TslearnModelFactory(),  SklearnModelFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "data_column_names = ['time', 'acceleration_x', 'acceleration_y', 'acceleration_z', #TODO: Pack in config/.env\n",
    "                             'orientation_w', 'orientation_x', 'orientation_y', 'orientation_z',\n",
    "                             'gravity_x', 'gravity_y', 'gravity_z'\n",
    "                             ]\n",
    "label_column_names = ['coarse_label', 'fine_label', 'road_label']\n",
    "\n",
    "#bad trips: 310517, 260417, 200617, 160517, 150317, 090517, 050517\n",
    "trips = [\n",
    "        '010317', '010617', '020317', \n",
    "        '020517', '020617', '030317', '030517', '030617', '030717',\n",
    "        '040517', '040717', '050617', '050717', '060317', '060617',\n",
    "        '070317', '070617', '080317', '080517', '080617', '090317', \n",
    "        '090617', '100317', '100517', '110517', '120517', '120617',\n",
    "        '130317', '130617', '140317', '140617', '150517', '150617', \n",
    "        '160317', '170317', '170517', '180417', '190417', '190517',\n",
    "        '200317', '200417', '200517', '210317', '220317', '220517', \n",
    "        '220617', '230317', '230517', '230617', '240417', '240517', \n",
    "        '250317', '250417', '250517', '260517', '260617', '270317',\n",
    "        '270417', '270617', '280317', '280417', '280617', '290317',\n",
    "        '290517', '290617', '300317', '300517', '300617'\n",
    "]\n",
    "\n",
    "#trips = random.sample(trips, len(trips)//2)\n",
    "\n",
    "data_string = \"./data_sets/sussex_huawei/User1/{}/Hips_Motion.txt\"\n",
    "label_string = \"./data_sets/sussex_huawei/User1/{}/Label.txt\"\n",
    "use_data_cols = [0,1,2,3,10,11,12,13,14,15,16]#4,5,6,7,8,9,17,18,19\n",
    "use_label_cols = [1, 2, 3]\n",
    "\n",
    "labels, data = dao.bulk_read_data(\n",
    "    file_path=[\n",
    "        data_string,\n",
    "        label_string\n",
    "    ],\n",
    "    identifiers=trips,\n",
    "    column_names=[\n",
    "        data_column_names,\n",
    "        label_column_names\n",
    "    ],\n",
    "    use_columns=[\n",
    "        use_data_cols,\n",
    "        use_label_cols\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "# 2.1 Convert unix time (ms) to date time\n",
    "data = preprocessor.convert_unix_to_datetime(data, column = 'time', unit = 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.2 Label data and remove NaNs\n",
    "data = preprocessor.label_data(data, labels)\n",
    "data = preprocessor.remove_nans(data, replacement_mode='del_row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Normalization\n",
    "acelerometer_columns = ['acceleration_x', 'acceleration_y', 'acceleration_z']\n",
    "gravity_columns = ['gravity_x', 'gravity_y', 'gravity_z']\n",
    "orientation_columns = ['orientation_x', 'orientation_y', 'orientation_z', 'orientation_w']\n",
    "\n",
    "data = preprocessor.project_accelerometer_to_global_coordinates(\n",
    "            data, \n",
    "            mode ='gravity', \n",
    "            target_columns = acelerometer_columns,\n",
    "            args = gravity_columns)\n",
    "\n",
    "data = preprocessor.project_accelerometer_to_global_coordinates(\n",
    "            data, \n",
    "            mode ='orientation', \n",
    "            target_columns = acelerometer_columns,\n",
    "            args = orientation_columns)\n",
    "\n",
    "\n",
    "#data = preprocessor.znormalize_quantitative_data(data, data_column_names[1:])\n",
    "#data = preprocessor.min_max_normalize_quantitative_data(data, data_column_names[1:])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Segment data\n",
    "# Coarse Label: Null=0, Still=1, Walking=2, Run=3, Bike=4, Car=5, Bus=6, Train=7, Subway=8\n",
    "# Road Label: City=1, Motorway=2, Countryside=3, Dirt road=4, Null=0\n",
    "selected_coarse_labels = [5]\n",
    "selected_road_labels = [1, 3]\n",
    "car_segments = preprocessor.segment_data(data, mode='labels', \n",
    "                                 label_column='coarse_label', \n",
    "                                 args=selected_coarse_labels)\n",
    "\n",
    "#print(car_segments)\n",
    "data_segments = []\n",
    "for car_segment in car_segments:\n",
    "        road_segments = preprocessor.segment_data(car_segment, mode='labels', \n",
    "                                  label_column='road_label',\n",
    "                                  args=selected_road_labels\n",
    "                                )\n",
    "        for road_segment in road_segments:\n",
    "            data_segments.append(road_segment)   \n",
    "            \n",
    "print(len(data_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Low Pass filtering -> #100 Hz to 40 Hz\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = data_segments[ind].set_index('time')\n",
    "    data_segments[ind] = preprocessor.resample_quantitative_data(data_segments[ind], freq='1000ms')\n",
    "    #current.1000ms 10 hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Outlier removal:\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = preprocessor.remove_outliers_from_quantitative_data(\n",
    "        data_segments[ind],\n",
    "        replacement_mode = 'quantile',\n",
    "        columns = acelerometer_columns,\n",
    "        quantile = 0.95 #current run @0.99\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Dimensionality reduction:\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = preprocessor.min_max_normalize_quantitative_data(\n",
    "    preprocessor.reduce_quantitativ_data_dimensionality(\n",
    "        data = data_segments[ind],\n",
    "        mode ='euclidean',\n",
    "        columns = acelerometer_columns,\n",
    "        reduced_column_name = 'acceleration_abs'\n",
    "    ), ['acceleration_abs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.8 Prepare for Basline Extractor\n",
    "selected_columns = ['acceleration_abs', 'road_label']\n",
    "data = preprocessor.de_segment_data(data_segments, selected_columns)\n",
    "#data = preprocessor.znormalize_quantitative_data(data, ['acceleration_abs'])\n",
    "#data = preprocessor.min_max_normalize_quantitative_data(data, ['acceleration_abs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visual anlaysis of the segments:\n",
    "#sns.set(rc={'figure.figsize':(15, 4)})\n",
    "#fig, ax = plt.subplots(figsize=(15,4*len(data_segments)), ncols=1, nrows=len(data_segments)+1)\n",
    "#for ind in range(len(data_segments)): \n",
    "#    sns.lineplot(y='acceleration_abs', x='time', data = data_segments[ind], ax=ax[ind])\n",
    "#    ax[ind].legend(\"Road\" if data_segments[ind]['road_label'].iloc[0] < 2.0 else \"City\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16, 6))\n",
    "sns.lineplot(data=data[['acceleration_abs', 'road_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extraction\n",
    "# 3.1 Encode categorical to binary\n",
    "data = preprocessor.encode_categorical_features(data = data, \n",
    "                                                mode = 'custom_function', \n",
    "                                                columns = ['road_label'],\n",
    "                                                encoding_function = lambda x :  (x  > 2.0).astype(int)\n",
    "                                               ) #0 City, 1 Countryside\n",
    "\n",
    "# 3.2\n",
    "# Generate label vector y and feature matrix X.\n",
    "# We need at least 2 classes to learn features for tsfresh\n",
    "y = data[['road_label']].reset_index(drop=True)\n",
    "data['id'] = range(1, len(data) + 1)\n",
    "y['id'] = data['id']\n",
    "y['road_label'].index=list(y['id'])\n",
    "\n",
    "# 3.3 Extract feature matrix\n",
    "# Read https://github.com/blue-yonder/tsfresh/issues/444 for info about the warnings\n",
    "X = extractor.extract_features(data = data, args = ['id', y['road_label'], 32, None, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88846\n"
     ]
    }
   ],
   "source": [
    "#3.3.1 Read/Write extracted features\n",
    "#dao.write_features('./data_sets/X.pkl', X)\n",
    "#dao.write_features('./data_sets/y.pkl', y)\n",
    "X = dao.load_features('./data_sets/X.pkl')\n",
    "y = dao.load_features('./data_sets/y.pkl')\n",
    "print(len(y))\n",
    "keys = X.keys()\n",
    "keys = list(filter(lambda x: \"acceleration_abs\" in x, keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# 3.4 combine feature rows\n",
    "X_join = pandas.concat([X, y], axis=1)\n",
    "X_join = preprocessor.remove_nans(X_join, replacement_mode='del_row')\n",
    "X_join[['road_label']] = X_join[['road_label']].astype('int')\n",
    "X_segments = preprocessor.segment_data(X_join, mode='labels', \n",
    "                                    label_column='road_label', \n",
    "                                    args=[0,1])\n",
    "\n",
    "\n",
    "segment_length = 20 #60s best in paper, 90 best in my evaluation, tested 30, 60, 90, 120\n",
    "X_segments_new = []\n",
    "for ind in range(0, len(X_segments)):\n",
    "    X_segments_new = X_segments_new + preprocessor.segment_data(\n",
    "        X_segments[ind],\n",
    "        mode = 'fixed_interval', \n",
    "        args = [segment_length, True, True]\n",
    "    )\n",
    "    \n",
    "    \n",
    "print(len(X_segments_new))\n",
    "keys.append('road_label')\n",
    "X_combined = preprocessor.de_segment_data(X_segments_new, keys)\n",
    "X_combined, y_combined = X_combined[keys[:-1]], X_combined[keys[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#X_combined.hist(figsize=(15,15)) #check ditrsibution -> normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.matshow(X_combined.corr(), fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.matshow(X_combined.cov(), fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3826, 20)\n",
      "0.5846837428123366\n",
      "(2104, 20)\n",
      "0.5960076045627376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.clustering import GlobalAlignmentKernelKMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 3.5 Read/Write combined features\n",
    "#print(type(y_combined))\n",
    "dao.write_features('./data_sets/X_combined.pkl', X_combined)\n",
    "dao.write_features('./data_sets/y_combined.pkl', y_combined)\n",
    "# 3.6  extracted features\n",
    "X_combined = dao.load_features('./data_sets/X_combined.pkl')\n",
    "y_combined = dao.load_features('./data_sets/y_combined.pkl')\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(list(y_combined).count(0)/len(y_combined))\n",
    "  \n",
    "y_clustering = IsolationForest(behaviour='new', \n",
    "                               max_samples=10, \n",
    "                               n_jobs=-1, \n",
    "                               contamination=0.45,\n",
    "                               max_features=1.0,\n",
    "                               n_estimators=1750\n",
    "                              ).fit_predict(X_combined)\n",
    "\n",
    "X_combined = X_combined.loc[pandas.DataFrame(y_clustering)[0] == 1]\n",
    "y_combined = y_combined.loc[pandas.DataFrame(y_clustering)[0] == 1]\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(list(y_combined).count(0)/len(y_combined))\n",
    "\n",
    "X_combined = X_combined.reset_index(drop=True)\n",
    "y_combined = y_combined.reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined,\n",
    "                                                    y_combined,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y_combined\n",
    "                                                    )\n",
    "\n",
    "#X_combined.hist(figsize=(15,15)) #check ditrsibution -> normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test)\n",
    "#num_cols = len(list(X_combined))\n",
    "#X_combined.columns = np.arange(num_cols)\n",
    "#X_combined  = (X_combined-X_combined.mean())/X_combined.std()\n",
    "\n",
    "#num_cols = len(list(X_test))\n",
    "#X_test.columns = np.arange(num_cols)\n",
    "#X_test = (X_test-X_test.mean())/X_test.std()\n",
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Sklearn-----------------\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Produce models from a given hyper parameter search space\n",
    "#TODO: Put model types and parametzer and search spaces into config.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('------------------Sklearn-----------------')\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'svc',\n",
    "    X = X_combined, \n",
    "    y = y_combined, \n",
    "    model_params = {\n",
    "        'kernel': ['rbf', 'linear','poly'],\n",
    "        'degree': sp_randint(2, X_combined.shape[1]*3),\n",
    "        'gamma': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'C': sp_randint(2, 5000),\n",
    "        'max_iter' : sp_randint(2, 5000),\n",
    "        'shrinking' : [True, False],\n",
    "        'probability' : [True, False],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"svc_rs.pickle\", 0.1]\n",
    "    )\n",
    "print('------------------SVC-----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'cart_tree',\n",
    "    X = X_combined, \n",
    "    y = y_combined, \n",
    "    model_params = {\n",
    "        \"max_depth\": sp_randint(1, 128),\n",
    "        \"max_features\": sp_randint(1, X_combined.shape[1]),\n",
    "        \"min_samples_leaf\": sp_randint(1, X_combined.shape[1]),\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"dt_rs.pickle\", 0.1]\n",
    "    )\n",
    "print('------------------CART-Tree-----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'random_forrest',\n",
    "    X = X_combined, \n",
    "    y = y_combined, \n",
    "    model_params = {\n",
    "        'n_estimators' : sp_randint(1, 100),\n",
    "        'max_depth': sp_randint(1, 128),\n",
    "        'max_features': sp_randint(1, X_combined.shape[1]),\n",
    "        'min_samples_split': sp_randint(2, X_combined.shape[1]),\n",
    "        'bootstrap': [True, False],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"rf_rs.pickle\", 0.1]\n",
    "    )\n",
    "print('------------------Random Forrest----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'mlp_classifier',\n",
    "    X = X_combined, \n",
    "    y = y_combined, \n",
    "    model_params = {\n",
    "        'solver': ['adam', 'lbfgs', 'sgd', 'adam'], \n",
    "        'max_iter': sp_randint(1, 250), \n",
    "        'alpha': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))), \n",
    "        'hidden_layer_sizes':[(128,128,128,128),\n",
    "                              (128,128,128),\n",
    "                              (128,128),\n",
    "                              (128),\n",
    "                              (64,64,64,64),\n",
    "                              (64,64,64),\n",
    "                              (64,64),\n",
    "                              (64),\n",
    "                              (32,32,32,32),\n",
    "                              (32,32,32),\n",
    "                              (32,32),\n",
    "                              (32),\n",
    "                              (16,16,16,16),\n",
    "                              (16,16,16),\n",
    "                              (16,16),\n",
    "                              (16)\n",
    "                             ], \n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'activation': [\"logistic\", \"relu\", \"tanh\"],\n",
    "        'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'learning_rate_init' : np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'batch_size' : sp_randint(1, 10),\n",
    "        'shuffle' :[True, False],\n",
    "        'early_stopping' : [True, False],\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 250, True, \"mlp_rs.pickle\", 0.1]\n",
    "    )\n",
    "print('------------------MLP----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "model = tslearn_factory.create_model(\n",
    "    model_type = 'tssvc',\n",
    "    X = X_combined, \n",
    "    y = y_combined, \n",
    "    model_params = {\n",
    "        'kernel': ['rbf', 'linear','poly', 'gak'],\n",
    "        'degree': sp_randint(2, X_combined.shape[1]*2),\n",
    "        'gamma': np.concatenate((10.0 ** -np.arange(0, 5),10.0 ** np.arange(1, 5))),\n",
    "        'max_iter' : sp_randint(2, 5),\n",
    "        'shrinking' : [True, False],\n",
    "        'probability' : [True, False],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "    },\n",
    "    search_params = [32, 0, 10, 250, True, \"tssv_rs.pickle\", 0.1]\n",
    "    )\n",
    "\n",
    "print('------------------Tslearn-----------------')\n",
    "print('------------------TSSVC----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = tslearn_factory.create_model(\n",
    "    model_type = 'knn_classifier',\n",
    "    X = X_combined, \n",
    "    y = y_combined, \n",
    "    model_params = {\n",
    "        'n_neighbors' : sp_randint(2, X_combined.shape[1]*2),\n",
    "        'metric' : ['dtw', 'softdtw', 'euclidean', 'sqeuclidean', 'cityblock']\n",
    "    },\n",
    "    search_params = [32, 0, 10, 250, True, \"tsknn_rs.pickle\", 0.1]\n",
    "    )\n",
    "print('------------------KNNC----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------sklearn----------------\")\n",
    "print(\"MLP\")\n",
    "with open('mlp_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.score(X_combined, y_combined))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"CART Tree\")\n",
    "with open('dt_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.score(X_combined, y_combined))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Random Forrest\")\n",
    "with open('rf_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.score(X_combined, y_combined))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"SVC\")\n",
    "with open('svc_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.score(X_combined, y_combined))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"----------------tslearn----------------\")\n",
    "print(\"TSSVC\")\n",
    "with open('tssv_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.score(X_combined, y_combined))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"TSKNN\")\n",
    "with open('tsknn_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(clf.score(X_combined, y_combined))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
