{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.data_access.dao.sussex_huawei_dao import SussexHuaweiDAO\n",
    "from pipeline.feature_engineering.preprocessing.sussex_huawei_preprocessor import SussexHuaweiPreprocessor\n",
    "from pipeline.feature_engineering.feature_extraction.baseline_extractor import BaselineExtractor\n",
    "from pipeline.feature_engineering.feature_extraction.mp_scrimp_extractor import MPScrimpExtractor\n",
    "from pipeline.machine_learning.model.sklearn_model_factory import SklearnModelFactory\n",
    "from pipeline.machine_learning.model.tslearn_model_factory import TslearnModelFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Initialize Pipeline Objects (TODO: Put into a pipleine Facade)\n",
    "dao = SussexHuaweiDAO()\n",
    "preprocessor = SussexHuaweiPreprocessor()\n",
    "extractor = BaselineExtractor()\n",
    "mp_extractor = MPScrimpExtractor()\n",
    "tslearn_factory, sklearn_factory = TslearnModelFactory(),  SklearnModelFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "data_column_names = ['time', 'acceleration_x', 'acceleration_y', 'acceleration_z', #TODO: Pack in config/.env\n",
    "                             #'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                             'orientation_w', 'orientation_x', 'orientation_y', 'orientation_z',\n",
    "                             'gravity_x', 'gravity_y', 'gravity_z',\n",
    "                             'l_acceleration_x', 'l_acceleration_y', 'l_acceleration_z',\n",
    "                             \n",
    "                             ]\n",
    "label_column_names = ['coarse_label', 'fine_label', 'road_label']\n",
    "\n",
    "#bad trips: 310517, 260417, 200617, 160517, 150317, 090517, 050517\n",
    "trips = [\n",
    "        '010317', '010617', '020317', \n",
    "        '020517', '020617', '030317', '030517', '030617', '030717',\n",
    "        '040517', '040717', '050617', '050717', '060317', '060617',\n",
    "        '070317', '070617', '080317', '080517', '080617', '090317', \n",
    "        '090617', '100317', '100517', '110517', '120517', '120617',\n",
    "        '130317', '130617', '140317', '140617', '150517', '150617', \n",
    "        '160317', '170317', '170517', '180417', '190417', '190517',\n",
    "        '200317', '200417', '200517', '210317', '220317', '220517', \n",
    "        '220617', '230317', '230517', '230617', '240417', '240517', \n",
    "        '250317', '250417', '250517', '260517', '260617', '270317',\n",
    "        '270417', '270617', '280317', '280417', '280617', '290317',\n",
    "        '290517', '290617', '300317', '300517', '300617'\n",
    "]\n",
    "\n",
    "#trips = random.sample(trips, len(trips)//2)\n",
    "\n",
    "data_string = \"./data_sets/sussex_huawei/User1/{}/Hips_Motion.txt\"\n",
    "label_string = \"./data_sets/sussex_huawei/User1/{}/Label.txt\"\n",
    "use_data_cols = [0,1,2,3, #4,5,6,7,8,9,\n",
    "                 10,11,12,13,14,15,16,\n",
    "                 17,18,19\n",
    "                ]#4,5,6,7,8,9,17,18,19\n",
    "#use_data_cols = [0,17,18,19,\n",
    "                 #10,11,12,13,14,15,16\n",
    "#                ]#4,5,6,7,8,9,17,18,19\n",
    "use_label_cols = [1, 2, 3]\n",
    "\n",
    "labels, data = dao.bulk_read_data(\n",
    "    file_path=[\n",
    "        data_string,\n",
    "        label_string\n",
    "    ],\n",
    "    identifiers=trips,\n",
    "    column_names=[\n",
    "        data_column_names,\n",
    "        label_column_names\n",
    "    ],\n",
    "    use_columns=[\n",
    "        use_data_cols,\n",
    "        use_label_cols\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "# 2.1 Convert unix time (ms) to date time\n",
    "data = preprocessor.convert_unix_to_datetime(data, column = 'time', unit = 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.2 Label data and remove NaNs\n",
    "data = preprocessor.label_data(data, labels)\n",
    "data = preprocessor.remove_nans(data, replacement_mode='del_row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Normalization\n",
    "acelerometer_columns = ['acceleration_x', 'acceleration_y', 'acceleration_z']\n",
    "gravity_columns = ['gravity_x', 'gravity_y', 'gravity_z']\n",
    "orientation_columns = ['orientation_x', 'orientation_y', 'orientation_z', 'orientation_w']\n",
    "\n",
    "#only used for motif discovery in n-d TS\n",
    "#data = preprocessor.project_accelerometer_to_global_coordinates(\n",
    "#            data, \n",
    "#            mode ='orientation', \n",
    "#            target_columns = acelerometer_columns,\n",
    "#            args = orientation_columns)\n",
    "\n",
    "\n",
    "#only used for motif discovery in n-d TS\n",
    "#data = preprocessor.project_accelerometer_to_global_coordinates(\n",
    "#            data, \n",
    "#            mode ='gravity', \n",
    "#            target_columns = acelerometer_columns,\n",
    "#           args = gravity_columns)\n",
    "\n",
    "#data = preprocessor.znormalize_quantitative_data(data, data_column_names[1:])\n",
    "#data = preprocessor.min_max_normalize_quantitative_data(data, data_column_names[1:])\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Segment data\n",
    "# Coarse Label: Null=0, Still=1, Walking=2, Run=3, Bike=4, Car=5, Bus=6, Train=7, Subway=8\n",
    "# Road Label: City=1, Motorway=2, Countryside=3, Dirt road=4, Null=0\n",
    "selected_coarse_labels = [5]\n",
    "selected_road_labels = [1, 3]\n",
    "car_segments = preprocessor.segment_data(data, mode='labels', \n",
    "                                 label_column='coarse_label', \n",
    "                                 args=selected_coarse_labels)\n",
    "\n",
    "#print(car_segments)\n",
    "data_segments = []\n",
    "for car_segment in car_segments:\n",
    "        road_segments = preprocessor.segment_data(car_segment, mode='labels', \n",
    "                                  label_column='road_label',\n",
    "                                  args=selected_road_labels\n",
    "                                )\n",
    "        for road_segment in road_segments:\n",
    "            data_segments.append(road_segment)   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store before filtering for later experimenting with different filters\n",
    "print(len(data_segments))\n",
    "import pickle\n",
    "with open('./tmp_segments', 'wb') as segments_file:\n",
    "    pickle.dump(data_segments, segments_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./tmp_segments', 'rb') as segments_file:\n",
    "    data_segments = pickle.load(segments_file)\n",
    "print(len(data_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Low Pass filtering -> #100 Hz to 10 Hz\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = data_segments[ind].set_index('time')\n",
    "    data_segments[ind] = preprocessor.resample_quantitative_data(data_segments[ind], \n",
    "                                                                 freq='1000ms') #8000 1.25 Hz\n",
    "    #1000ms 10 hz used for tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_segments = data_segments[:int(len(data_segments)*0.15)] #make set smaller for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Dimensionality reduction:\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = preprocessor.reduce_quantitativ_data_dimensionality(\n",
    "                data = data_segments[ind],\n",
    "                mode ='euclidean', #works better than euclidean for motif\n",
    "                columns = acelerometer_columns,\n",
    "                reduced_column_name = 'acceleration_abs'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122398, 2)\n"
     ]
    }
   ],
   "source": [
    "#2.8 Prepare for Extractor\n",
    "selected_columns = ['acceleration_abs',\n",
    "                    'road_label'] #'acceleration_abs'\n",
    "data = preprocessor.de_segment_data(data_segments, selected_columns)\n",
    "data = preprocessor.znormalize_quantitative_data(data, selected_columns[:-1])\n",
    "#data = preprocessor.min_max_normalize_quantitative_data(data, selected_columns[:-1])\n",
    "print(data.shape)\n",
    "\n",
    "data = preprocessor.remove_outliers_from_quantitative_data(\n",
    "        data,\n",
    "        replacement_mode = 'quantile',\n",
    "        columns = selected_columns[:-1],\n",
    "        quantile = 0.99 #current run @0.95 for classical approach via TS Fresh\n",
    "    )[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker #0\n",
      "Worker #5\n",
      "Worker #2\n",
      "Worker #6\n",
      "Worker #7\n",
      "Worker #8\n",
      "Worker #4\n",
      "Worker #11\n",
      "Worker #3\n",
      "Worker #10\n",
      "Worker #1\n",
      "Worker #13\n",
      "Worker #12\n",
      "Worker #14\n",
      "Worker #9\n",
      "Worker #15\n",
      "Worker #16\n",
      "Worker #21\n",
      "Worker #20\n",
      "Worker #18\n",
      "Worker #27\n",
      "Worker #19\n",
      "Worker #22\n",
      "Worker #17\n",
      "Worker #26\n",
      "Worker #24\n",
      "Worker #23\n",
      "Worker #28\n",
      "Worker #25\n"
     ]
    }
   ],
   "source": [
    "#Find different combinations of hyperparameters\n",
    "from multiprocessing import Pool\n",
    "def worker(i):\n",
    "    print('Worker #{}'.format(i))\n",
    "    combis = []\n",
    "    radii = [8, 12, 16, 20, 24, 32] #6\n",
    "    lengths = [6, 12, 18, 24, 32] #5\n",
    "    for radius in radii:\n",
    "        for length in lengths:\n",
    "            combi = [radius, length]\n",
    "            combis.append(combi)\n",
    "\n",
    "    X_indices = mp_extractor.extract_features(data = data, \n",
    "                                     args = [combis[i][1], 2, combis[i][0], 'acceleration_abs'])\n",
    "    X = mp_extractor.select_features(data = data, \n",
    "                                         args = [combis[i][1], 2, X_indices, 'acceleration_abs'])\n",
    "    y = mp_extractor.select_features(data = data, \n",
    "                                         args = [combis[i][1], 1, X_indices, 'road_label'])\n",
    "    \n",
    "    return {\n",
    "            'X' : X, \n",
    "            'y': y,\n",
    "            'radius' : combis[i][0],\n",
    "            'length' : combis[i][1],\n",
    "            'motifs' : 2\n",
    "           }\n",
    "\n",
    "num_processors = 32                         #create a pool of processors\n",
    "p = Pool(processes = num_processors)        #get them to work in parallel#\n",
    "output = p.map(worker,[i for i in range(0,29)])\n",
    "        \n",
    "\n",
    "result_list = []\n",
    "result_list.append(output[0].keys())\n",
    "for elem in output:\n",
    "    templist = []\n",
    "    for key in elem.keys():\n",
    "        templist.append(elem[key])\n",
    "    result_list.append(templist)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for i in range(1, len(result_list)):\n",
    "    X = result_list[i][0]\n",
    "    y = result_list[i][1]\n",
    "    X_valid = X.tail(int(len(X)*0.3))\n",
    "    y_valid = y.tail(int(len(X)*0.3))\n",
    "    X_train = X.head(len(X)-int(len(X)*0.3))\n",
    "    y_train = y.head(len(X)-int(len(X)*0.3))\n",
    "    if int(len(X)*0.3) < 100:\n",
    "        continue\n",
    "    \n",
    "    #Test SVC on motif discovery\n",
    "    print('------------------Sklearn-----------------')\n",
    "    model = sklearn_factory.create_model(\n",
    "        model_type = 'svc',\n",
    "        X = X_train, \n",
    "        y = y_train, \n",
    "        model_params = {\n",
    "            'kernel': ['rbf', 'linear','poly'],\n",
    "            'degree': sp_randint(2, X_train.shape[1]*3),\n",
    "            'gamma': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "            'C': sp_randint(2, 5000),\n",
    "            'max_iter' : sp_randint(2, 5000),\n",
    "            'shrinking' : [True, False],\n",
    "            'probability' : [True, False],\n",
    "            'random_state': sp_randint(1, 10),\n",
    "        },\n",
    "        search_params = [-1, 0, 10, 250, True, \"svc_rs.pickle\", 0.2]\n",
    "        )\n",
    "    print('------------------SVC-----------------')\n",
    "    X_test, y_test = model['X_test'], model['y_test']\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    model = sklearn_factory.create_model(\n",
    "    model_type = 'cart_tree',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        \"max_depth\": sp_randint(1, 128),\n",
    "        \"max_features\": sp_randint(1, X_train.shape[1]),\n",
    "        \"min_samples_leaf\": sp_randint(1, X_train.shape[1]),\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 250, True, \"dt_rs.pickle\", 0.2]\n",
    "    )\n",
    "    print('------------------CART-Tree-----------------')\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    model = sklearn_factory.create_model(\n",
    "    model_type = 'mlp_classifier',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'solver': ['adam', 'lbfgs', 'sgd'], \n",
    "        'max_iter': sp_randint(1, 250), \n",
    "        'alpha': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))), \n",
    "        'hidden_layer_sizes':[(128,128,128,128), #architecture see\n",
    "                              (128,128,128),\n",
    "                              (128,128),\n",
    "                              (128),\n",
    "                              (64,64,64,64),\n",
    "                              (64,64,64),\n",
    "                              (64,64),\n",
    "                              (64),\n",
    "                              (32,32,32,32),\n",
    "                              (32,32,32),\n",
    "                              (32,32),\n",
    "                              (32),\n",
    "                              (16,16,16,16),\n",
    "                              (16,16,16),\n",
    "                              (16,16),\n",
    "                              (16)\n",
    "                             ], \n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'activation': [\"logistic\", \"relu\", \"tanh\"],\n",
    "        'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'learning_rate_init' : np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'batch_size' : sp_randint(1, 10),\n",
    "        'shuffle' :[True, False],\n",
    "        'early_stopping' : [True, False],\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 25, True, \"mlp_rs.pickle\", 0.2]\n",
    "    )\n",
    "    print('------------------MLP----------------')\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "    model = sklearn_factory.create_model(\n",
    "        model_type = 'random_forrest',\n",
    "        X = X_train, \n",
    "        y = y_train,\n",
    "        model_params = {\n",
    "            'n_estimators' : sp_randint(1, 100),\n",
    "            'max_depth': sp_randint(1, 128),\n",
    "            #'max_features': sp_randint(1, X_train.shape[1]),\n",
    "            'min_samples_split': sp_randint(2, X_train.shape[1]),\n",
    "            'bootstrap': [True, False],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            'random_state': sp_randint(1, 10),\n",
    "            'min_samples_split': sp_randint(2, 10)\n",
    "        },\n",
    "        search_params = [-1, 0, 10, 100, True, \"rf_rs.pickle\", 0.2]\n",
    "        )\n",
    "    print('------------------Random Forrest----------------')\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Store in correct format for Sequitur motif discovery\n",
    "motif_data=data.reindex(columns=['road_label', \n",
    "                                 #'acceleration_abs',\n",
    "                                 #'acceleration_abs_2'\n",
    "                                 #'acceleration_x', 'acceleration_y', 'acceleration_z',\n",
    "                                 #'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                                 #'gravity_x', 'gravity_y', 'gravity_z',\n",
    "                                 #'orientation_x', 'orientation_y', 'orientation_z', 'orientation_w',\n",
    "                                 'l_acceleration_x', 'l_acceleration_y', 'l_acceleration_z',\n",
    "                    \n",
    "                                ]).astype('float') #'acceleration_abs'\n",
    "motif_data['road_label']=motif_data['road_label'].astype('int') \n",
    "\n",
    "motif_data = preprocessor.remove_nans(motif_data, replacement_mode='del_row')\n",
    "test_sz = 0.1\n",
    "#print(motif_data)\n",
    "print(list(motif_data['road_label']).count(1.0)/len(motif_data))\n",
    "print(list(motif_data['road_label']).count(3.0)/len(motif_data))\n",
    "\n",
    "motif_data_test = motif_data.head(int(len(motif_data)*test_sz))\n",
    "motif_data_train = motif_data.tail(len(motif_data)-int(len(motif_data)*test_sz))\n",
    "print(motif_data_test.shape)\n",
    "print(motif_data_train.shape)\n",
    "print(list(motif_data_train['road_label']).count(1.0)/len(motif_data_train))\n",
    "print(list(motif_data_train['road_label']).count(3.0)/len(motif_data_train))\n",
    "print(list(motif_data_test['road_label']).count(1.0)/len(motif_data_test))\n",
    "print(list(motif_data_test['road_label']).count(3.0)/len(motif_data_test))\n",
    "\n",
    "#np.savetxt(r'./motif_data_test.txt', motif_data_test.values, fmt='%1.6f')\n",
    "#np.savetxt(r'./motif_data_train.txt', motif_data_train.values, fmt='%1.6f')\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#print(motif_data_test)\n",
    "\n",
    "#motif_data_train_small = motif_data.tail(int(len(motif_data)*0.5)-int(len(motif_data)*test_sz*0.5))\n",
    "#motif_data_test_small = motif_data.head(int(len(motif_data)*test_sz*0.5))\n",
    "#print(motif_data_test_small.shape)\n",
    "#print(motif_data_train_small.shape)\n",
    "#print(list(motif_data_train_small['road_label']).count(1.0)/len(motif_data_train_small))\n",
    "#print(list(motif_data_train_small['road_label']).count(3.0)/len(motif_data_train_small))\n",
    "#print(list(motif_data_test_small['road_label']).count(1.0)/len(motif_data_test_small))\n",
    "#print(list(motif_data_test_small['road_label']).count(3.0)/len(motif_data_test_small))\n",
    "\n",
    "#print(motif_data_test_small)\n",
    "#np.savetxt(r'./motif_data_test_small.txt', motif_data_test_small.values, fmt='%1.4f')\n",
    "#np.savetxt(r'./motif_data_train_small.txt', motif_data_train_small.values, fmt='%1.4f')\n",
    "#np.savetxt(r'./X_Train.txt', motif_data_train['acceleration_abs'].values, fmt='%1.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visual anlaysis of the segments:\n",
    "#sns.set(rc={'figure.figsize':(15, 4)})\n",
    "#fig, ax = plt.subplots(figsize=(15,4*len(data_segments)), ncols=1, nrows=len(data_segments)+1)\n",
    "#for ind in range(len(data_segments)): \n",
    "#    sns.lineplot(y='acceleration_abs', x='time', data = data_segments[ind], ax=ax[ind])\n",
    "#    ax[ind].legend(\"Road\" if data_segments[ind]['road_label'].iloc[0] < 2.0 else \"City\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16, 6))\n",
    "#sns.lineplot(data=motif_data_train[['acceleration_abs', 'road_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16, 6))\n",
    "#sns.lineplot(data=motif_data_train[['l_acceleration_y', 'road_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extraction\n",
    "# 3.1 Encode categorical to binary\n",
    "data = preprocessor.encode_categorical_features(data = data, \n",
    "                                                mode = 'custom_function', \n",
    "                                                columns = ['road_label'],\n",
    "                                                encoding_function = lambda x :  (x  > 2.0).astype(int)\n",
    "                                               ) #0 City, 1 Countryside\n",
    "\n",
    "# 3.2\n",
    "# Generate label vector y and feature matrix X.\n",
    "# We need at least 2 classes to learn features for tsfresh\n",
    "y = data[['road_label']].reset_index(drop=True)\n",
    "data['id'] = range(1, len(data) + 1)\n",
    "y['id'] = data['id']\n",
    "y['road_label'].index=list(y['id'])\n",
    "\n",
    "# 3.3 Extract feature matrix\n",
    "# Read https://github.com/blue-yonder/tsfresh/issues/444 for info about the warnings\n",
    "#if task is just inference, use extractor and select features found relevant during training.\n",
    "X = extractor.extract_features(data = data, args = ['id', 32, None]) \n",
    "X = extractor.select_features(data = X, args = [y['road_label'], 32, None, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3.3.1 Read/Write extracted features\n",
    "#dao.write_features('./data_sets/X.pkl', X)\n",
    "#dao.write_features('./data_sets/y.pkl', y)\n",
    "X = dao.load_features('./data_sets/X.pkl')\n",
    "y = dao.load_features('./data_sets/y.pkl')\n",
    "print(len(y))\n",
    "keys = X.keys()\n",
    "keys = list(filter(lambda x: \"acceleration_abs\" in x, keys))\n",
    "print(X.shape)\n",
    "#print(y)\n",
    "print(list(y['road_label']).count(0)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 combine feature rows\n",
    "X_join = pandas.concat([X, y], axis=1)\n",
    "X_join = preprocessor.remove_nans(X_join, replacement_mode='del_row')\n",
    "X_join[['road_label']] = X_join[['road_label']].astype('int')\n",
    "X_segments = preprocessor.segment_data(X_join, mode='labels', \n",
    "                                    label_column='road_label', \n",
    "                                    args=[0,1])\n",
    "\n",
    "\n",
    "segment_length = 30 #60s best in paper, 90 best in my evaluation, tested 30, 60, 90, 120\n",
    "X_segments_new = []\n",
    "for ind in range(0, len(X_segments)):\n",
    "    X_segments_new = X_segments_new + preprocessor.segment_data(\n",
    "        X_segments[ind],\n",
    "        mode = 'fixed_interval', \n",
    "        args = [segment_length, True, True]\n",
    "    )\n",
    "    \n",
    "    \n",
    "print(len(X_segments_new))\n",
    "keys.append('road_label')\n",
    "X_combined = preprocessor.de_segment_data(X_segments_new, keys)\n",
    "X_combined, y_combined = X_combined[keys[:-1]], X_combined[keys[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#X_combined.hist(figsize=(15,15)) #check ditrsibution -> normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.matshow(X_combined.corr(), fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.matshow(X_combined.cov(), fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 3.5 Read/Write combined features\n",
    "#print(type(y_combined))\n",
    "dao.write_features('./data_sets/X_combined.pkl', X_combined)\n",
    "dao.write_features('./data_sets/y_combined.pkl', y_combined)\n",
    "# 3.6  extracted features\n",
    "X_combined = dao.load_features('./data_sets/X_combined.pkl')\n",
    "y_combined = dao.load_features('./data_sets/y_combined.pkl')\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(list(y_combined).count(0)/len(y_combined))\n",
    "  \n",
    "y_clustering = IsolationForest(behaviour='new', \n",
    "                               max_samples=5, \n",
    "                               n_jobs=-1, \n",
    "                               contamination=0.25,\n",
    "                               max_features=1.0,\n",
    "                               n_estimators=750\n",
    "                              ).fit_predict(X_combined)\n",
    "\n",
    "X_combined = X_combined.loc[pandas.DataFrame(y_clustering)[0] == 1]\n",
    "y_combined = y_combined.loc[pandas.DataFrame(y_clustering)[0] == 1]\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(list(y_combined).count(0)/len(y_combined))\n",
    "\n",
    "X_combined = X_combined.reset_index(drop=True)\n",
    "y_combined = y_combined.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#X_combined.hist(figsize=(15,15)) #check ditrsibution -> normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried but failed improvbement methods\n",
    "#1\n",
    "#https://en.wikipedia.org/wiki/Feature_selection#Correlation_feature_selection\n",
    "#X_combined = X_combined[X_combined.columns[[0,1,3,20,21,22]]\n",
    "# Didnt help\n",
    "\n",
    "# Preclustering using DBSCAN, Optics and KMeans (last on normalized Dataset) and adding prediciton \n",
    "# as new feature made results worse.\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_combined,\n",
    "                                                    y_combined,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y_combined\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4.1 Produce models from a given hyper parameter search space\n",
    "#TODO: Put model types and parametzer and search spaces into config.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('------------------Sklearn-----------------')\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'svc',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'kernel': ['rbf', 'linear','poly'],\n",
    "        'degree': sp_randint(2, X_combined.shape[1]*3),\n",
    "        'gamma': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'C': sp_randint(2, 5000),\n",
    "        'max_iter' : sp_randint(2, 5000),\n",
    "        'shrinking' : [True, False],\n",
    "        'probability' : [True, False],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"svc_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------SVC-----------------')\n",
    "X_test, y_test = model['X_test'], model['y_test']\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'cart_tree',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        \"max_depth\": sp_randint(1, 128),\n",
    "        \"max_features\": sp_randint(1, X_combined.shape[1]),\n",
    "        \"min_samples_leaf\": sp_randint(1, X_combined.shape[1]),\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"dt_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------CART-Tree-----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'random_forrest',\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    model_params = {\n",
    "        'n_estimators' : sp_randint(1, 100),\n",
    "        'max_depth': sp_randint(1, 128),\n",
    "        'max_features': sp_randint(1, X_combined.shape[1]),\n",
    "        'min_samples_split': sp_randint(2, X_combined.shape[1]),\n",
    "        'bootstrap': [True, False],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"rf_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------Random Forrest----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'mlp_classifier',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'solver': ['adam', 'lbfgs', 'sgd'], \n",
    "        'max_iter': sp_randint(1, 250), \n",
    "        'alpha': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))), \n",
    "        'hidden_layer_sizes':[(128,128,128,128), #architecture see\n",
    "                              (128,128,128),\n",
    "                              (128,128),\n",
    "                              (128),\n",
    "                              (64,64,64,64),\n",
    "                              (64,64,64),\n",
    "                              (64,64),\n",
    "                              (64),\n",
    "                              (32,32,32,32),\n",
    "                              (32,32,32),\n",
    "                              (32,32),\n",
    "                              (32),\n",
    "                              (16,16,16,16),\n",
    "                              (16,16,16),\n",
    "                              (16,16),\n",
    "                              (16)\n",
    "                             ], \n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'activation': [\"logistic\", \"relu\", \"tanh\"],\n",
    "        'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'learning_rate_init' : np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'batch_size' : sp_randint(1, 10),\n",
    "        'shuffle' :[True, False],\n",
    "        'early_stopping' : [True, False],\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 100, True, \"mlp_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------MLP----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "model = tslearn_factory.create_model(\n",
    "    model_type = 'tssvc',\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    model_params = {\n",
    "        'kernel': ['rbf', 'linear','poly', 'gak'],\n",
    "        'degree': sp_randint(2, X_combined.shape[1]*3),\n",
    "        'gamma': np.concatenate((10.0 ** -np.arange(0, 5),10.0 ** np.arange(1, 10))),\n",
    "        'max_iter' : sp_randint(2, 5000),\n",
    "        'shrinking' : [True, False],\n",
    "        'probability' : [True, False],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "    },\n",
    "    search_params = [32, 0, 10, 250, True, \"tssv_rs.pickle\", 0.2]\n",
    "    )\n",
    "\n",
    "print('------------------Tslearn-----------------')\n",
    "print('------------------TSSVC----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = tslearn_factory.create_model(\n",
    "    model_type = 'knn_classifier',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'n_neighbors' : sp_randint(2, X_combined.shape[1]*2),\n",
    "        'metric' : ['dtw', 'softdtw', 'euclidean', 'sqeuclidean', 'cityblock']\n",
    "    },\n",
    "    search_params = [32, 0, 10, 250, True, \"tsknn_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------KNNC----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Evaluate best model using validation set\n",
    "print('Testset:')\n",
    "print('Shape: '+str(X_test.shape))\n",
    "print('% class 0 (City)'+str(list(y_test).count(0)/len(y_test)))\n",
    "print('\\n')\n",
    "print('Validationset:')\n",
    "print('Shape: '+str(X_validation.shape))\n",
    "print('% class 0 (City)'+str(list(y_validation).count(0)/len(y_validation)))\n",
    "print('\\n')\n",
    "\n",
    "print(\"----------------sklearn----------------\")\n",
    "print(\"MLP\")\n",
    "with open('mlp_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"CART Tree\")\n",
    "with open('dt_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Random Forrest\")\n",
    "with open('rf_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"SVC\")\n",
    "with open('svc_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"----------------tslearn----------------\")\n",
    "print(\"TSSVC\")\n",
    "with open('tssv_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"TSKNN\")\n",
    "with open('tsknn_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
