{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.data_access.dao.sussex_huawei_dao import SussexHuaweiDAO\n",
    "from pipeline.feature_engineering.preprocessing.sussex_huawei_preprocessor import SussexHuaweiPreprocessor\n",
    "from pipeline.feature_engineering.feature_extraction.baseline_extractor import BaselineExtractor\n",
    "from pipeline.feature_engineering.feature_extraction.mp_scrimp_extractor import MPScrimpExtractor\n",
    "from pipeline.machine_learning.model.sklearn_model_factory import SklearnModelFactory\n",
    "from pipeline.machine_learning.model.tslearn_model_factory import TslearnModelFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Initialize Pipeline Objects (TODO: Put into a pipleine Facade)\n",
    "dao = SussexHuaweiDAO()\n",
    "preprocessor = SussexHuaweiPreprocessor()\n",
    "extractor = BaselineExtractor()\n",
    "mp_extractor = MPScrimpExtractor()\n",
    "tslearn_factory, sklearn_factory = TslearnModelFactory(),  SklearnModelFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "data_column_names = ['time', 'acceleration_x', 'acceleration_y', 'acceleration_z', #TODO: Pack in config/.env\n",
    "                             #'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                             'orientation_w', 'orientation_x', 'orientation_y', 'orientation_z',\n",
    "                             'gravity_x', 'gravity_y', 'gravity_z',\n",
    "                             'l_acceleration_x', 'l_acceleration_y', 'l_acceleration_z',\n",
    "                             \n",
    "                             ]\n",
    "label_column_names = ['coarse_label', 'fine_label', 'road_label']\n",
    "\n",
    "#bad trips: 310517, 260417, 200617, 160517, 150317, 090517, 050517\n",
    "trips = [\n",
    "        '010317', '010617', '020317', \n",
    "        '020517', '020617', '030317', '030517', '030617', '030717',\n",
    "        '040517', '040717', '050617', '050717', '060317', '060617',\n",
    "        '070317', '070617', '080317', '080517', '080617', '090317', \n",
    "        '090617', '100317', '100517', '110517', '120517', '120617',\n",
    "        '130317', '130617', '140317', '140617', '150517', '150617', \n",
    "        '160317', '170317', '170517', '180417', '190417', '190517',\n",
    "        '200317', '200417', '200517', '210317', '220317', '220517', \n",
    "        '220617', '230317', '230517', '230617', '240417', '240517', \n",
    "        '250317', '250417', '250517', '260517', '260617', '270317',\n",
    "        '270417', '270617', '280317', '280417', '280617', '290317',\n",
    "        '290517', '290617', '300317', '300517', '300617'\n",
    "]\n",
    "\n",
    "#trips = random.sample(trips, len(trips)//2)\n",
    "\n",
    "data_string = \"./data_sets/sussex_huawei/User1/{}/Hips_Motion.txt\"\n",
    "label_string = \"./data_sets/sussex_huawei/User1/{}/Label.txt\"\n",
    "use_data_cols = [0,1,2,3, #4,5,6,7,8,9,\n",
    "                 10,11,12,13,14,15,16,\n",
    "                 17,18,19\n",
    "                ]#4,5,6,7,8,9,17,18,19\n",
    "#use_data_cols = [0,17,18,19,\n",
    "                 #10,11,12,13,14,15,16\n",
    "#                ]#4,5,6,7,8,9,17,18,19\n",
    "use_label_cols = [1, 2, 3]\n",
    "\n",
    "labels, data = dao.bulk_read_data(\n",
    "    file_path=[\n",
    "        data_string,\n",
    "        label_string\n",
    "    ],\n",
    "    identifiers=trips,\n",
    "    column_names=[\n",
    "        data_column_names,\n",
    "        label_column_names\n",
    "    ],\n",
    "    use_columns=[\n",
    "        use_data_cols,\n",
    "        use_label_cols\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "# 2.1 Convert unix time (ms) to date time\n",
    "data = preprocessor.convert_unix_to_datetime(data, column = 'time', unit = 'ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.2 Label data and remove NaNs\n",
    "data = preprocessor.label_data(data, labels)\n",
    "data = preprocessor.remove_nans(data, replacement_mode='del_row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Normalization\n",
    "acelerometer_columns = ['acceleration_x', 'acceleration_y', 'acceleration_z']\n",
    "gravity_columns = ['gravity_x', 'gravity_y', 'gravity_z']\n",
    "orientation_columns = ['orientation_x', 'orientation_y', 'orientation_z', 'orientation_w']\n",
    "\n",
    "#only used for motif discovery in n-d TS\n",
    "#data = preprocessor.project_accelerometer_to_global_coordinates(\n",
    "#            data, \n",
    "#            mode ='orientation', \n",
    "#            target_columns = acelerometer_columns,\n",
    "#            args = orientation_columns)\n",
    "\n",
    "\n",
    "#only used for motif discovery in n-d TS\n",
    "#data = preprocessor.project_accelerometer_to_global_coordinates(\n",
    "#            data, \n",
    "#            mode ='gravity', \n",
    "#            target_columns = acelerometer_columns,\n",
    "#           args = gravity_columns)\n",
    "\n",
    "#data = preprocessor.znormalize_quantitative_data(data, data_column_names[1:])\n",
    "#data = preprocessor.min_max_normalize_quantitative_data(data, data_column_names[1:])\n",
    "#print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Segment data\n",
    "# Coarse Label: Null=0, Still=1, Walking=2, Run=3, Bike=4, Car=5, Bus=6, Train=7, Subway=8\n",
    "# Road Label: City=1, Motorway=2, Countryside=3, Dirt road=4, Null=0\n",
    "selected_coarse_labels = [5]\n",
    "selected_road_labels = [1, 3]\n",
    "car_segments = preprocessor.segment_data(data, mode='labels', \n",
    "                                 label_column='coarse_label', \n",
    "                                 args=selected_coarse_labels)\n",
    "\n",
    "#print(car_segments)\n",
    "data_segments = []\n",
    "for car_segment in car_segments:\n",
    "        road_segments = preprocessor.segment_data(car_segment, mode='labels', \n",
    "                                  label_column='road_label',\n",
    "                                  args=selected_road_labels\n",
    "                                )\n",
    "        for road_segment in road_segments:\n",
    "            data_segments.append(road_segment)   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store before filtering for later experimenting with different filters\n",
    "print(len(data_segments))\n",
    "import pickle\n",
    "with open('./tmp_segments', 'wb') as segments_file:\n",
    "    pickle.dump(data_segments, segments_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./tmp_segments', 'rb') as segments_file:\n",
    "    data_segments = pickle.load(segments_file)\n",
    "print(len(data_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 Low Pass filtering -> #100 Hz to 10 Hz\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = data_segments[ind].set_index('time')\n",
    "    data_segments[ind] = preprocessor.resample_quantitative_data(data_segments[ind], \n",
    "                                                                 freq='1000ms') #8000 1.25 Hz\n",
    "    #1000ms 10 hz used for tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_segments = data_segments[:int(len(data_segments)*0.25)] #make set smaller for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Dimensionality reduction:\n",
    "for ind in range(len(data_segments)):\n",
    "    data_segments[ind] = preprocessor.reduce_quantitativ_data_dimensionality(\n",
    "                data = data_segments[ind],\n",
    "                mode ='euclidean', #works better than euclidean for motif\n",
    "                columns = acelerometer_columns,\n",
    "                reduced_column_name = 'acceleration_abs'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122398, 2)\n"
     ]
    }
   ],
   "source": [
    "#2.8 Prepare for Extractor\n",
    "selected_columns = ['acceleration_abs',\n",
    "                    'road_label'] #'acceleration_abs'\n",
    "data = preprocessor.de_segment_data(data_segments, selected_columns)\n",
    "data = preprocessor.znormalize_quantitative_data(data, selected_columns[:-1])\n",
    "#data = preprocessor.min_max_normalize_quantitative_data(data, selected_columns[:-1])\n",
    "print(data.shape)\n",
    "\n",
    "data = preprocessor.remove_outliers_from_quantitative_data(\n",
    "        data,\n",
    "        replacement_mode = 'quantile',\n",
    "        columns = selected_columns[:-1],\n",
    "        quantile = 0.99 #current run @0.95 for classical approach via TS Fresh\n",
    "    )[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motif extraction worker no: 0 length: 6, radius: 8\n",
      "Motif extraction worker no: 2 length: 18, radius: 8\n",
      "Motif extraction worker no: 3 length: 24, radius: 8\n",
      "Motif extraction worker no: 1 length: 12, radius: 8\n",
      "Motif extraction worker no: 4 length: 32, radius: 8\n",
      "Motif extraction worker no: 8 length: 24, radius: 12\n",
      "Motif extraction worker no: 5 length: 6, radius: 12\n",
      "Motif extraction worker no: 6 length: 12, radius: 12\n",
      "Motif extraction worker no: 7 length: 18, radius: 12\n",
      "Motif extraction worker no: 9 length: 32, radius: 12\n",
      "Motif extraction worker no: 12 length: 18, radius: 16\n",
      "Motif extraction worker no: 11 length: 12, radius: 16\n",
      "Motif extraction worker no: 10 length: 6, radius: 16\n",
      "Motif extraction worker no: 13 length: 24, radius: 16\n",
      "Motif extraction worker no: 14 length: 32, radius: 16\n",
      "Motif extraction worker no: 15 length: 6, radius: 20\n",
      "Motif extraction worker no: 17 length: 18, radius: 20\n",
      "Motif extraction worker no: 21 length: 12, radius: 24\n",
      "Motif extraction worker no: 18 length: 24, radius: 20\n",
      "Motif extraction worker no: 16 length: 12, radius: 20\n",
      "Motif extraction worker no: 19 length: 32, radius: 20\n",
      "Motif extraction worker no: 23 length: 24, radius: 24\n",
      "Motif extraction worker no: 20 length: 6, radius: 24\n",
      "Motif extraction worker no: 24 length: 32, radius: 24\n",
      "Motif extraction worker no: 22 length: 18, radius: 24\n",
      "Motif extraction worker no: 27 length: 18, radius: 32\n",
      "Motif extraction worker no: 26 length: 12, radius: 32\n",
      "Motif extraction worker no: 25 length: 6, radius: 32\n",
      "Motif extraction worker no: 28 length: 24, radius: 32\n",
      "Motif extraction worker no: 14 returned\n",
      "Motif extraction worker no: 19 returned\n",
      "Motif extraction worker no: 4 returned\n",
      "Motif extraction worker no: 24 returned\n",
      "Motif extraction worker no: 9 returned\n",
      "Motif extraction worker no: 3 returned\n",
      "Motif extraction worker no: 17 returned\n",
      "Motif extraction worker no: 23 returned\n",
      "Motif extraction worker no: 28 returned\n",
      "Motif extraction worker no: 13 returned\n",
      "Motif extraction worker no: 8 returned\n",
      "Motif extraction worker no: 18 returned\n",
      "Motif extraction worker no: 7 returned\n",
      "Motif extraction worker no: 2 returned\n",
      "Motif extraction worker no: 27 returned\n",
      "Motif extraction worker no: 22 returned\n",
      "Motif extraction worker no: 12 returned\n",
      "Motif extraction worker no: 11 returned\n",
      "Motif extraction worker no: 26 returned\n",
      "Motif extraction worker no: 6 returned\n",
      "Motif extraction worker no: 16 returned\n",
      "Motif extraction worker no: 21 returned\n",
      "Motif extraction worker no: 1 returned\n",
      "Motif extraction worker no: 5 returned\n",
      "Motif extraction worker no: 10 returned\n",
      "Motif extraction worker no: 25 returned\n",
      "Motif extraction worker no: 20 returned\n",
      "Motif extraction worker no: 15 returned\n",
      "Motif extraction worker no: 0 returned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-22:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/lorenz/miniconda3/envs/rctc/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#Find different combinations of hyperparameters\n",
    "\n",
    "data_valid = data.tail(int(len(data)*0.3)) \n",
    "#we want to test calssification accuracy if we extract it \n",
    "#at different time than training set that s why we split here\n",
    "\n",
    "data_train = data.head(len(data)-len(data_valid))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "def worker(i):\n",
    "    combis = []\n",
    "    radii = [8, 12, 16, 20, 24, 32] #6\n",
    "    lengths = [6, 12, 18, 24, 32] #5\n",
    "    for radius in radii:\n",
    "        for length in lengths:\n",
    "            combi = [radius, length]\n",
    "            combis.append(combi)\n",
    "    print(\"Motif extraction worker no: {0} length: {1}, radius: {2}\".format(i, combis[i][1], combis[i][0]))\n",
    "    X_indices = mp_extractor.extract_features(data = data_train, \n",
    "                                     args = [combis[i][1], 2, combis[i][0], 'acceleration_abs'])\n",
    "    X = mp_extractor.select_features(data = data_train, \n",
    "                                         args = [combis[i][1], 2, X_indices, 'acceleration_abs'])\n",
    "    y = mp_extractor.select_features(data = data_train, \n",
    "                                         args = [combis[i][1], 1, X_indices, 'road_label'])\n",
    "    \n",
    "    print(\"Motif extraction worker no: {0} returned\".format(i))\n",
    "    return {\n",
    "            'X' : X, \n",
    "            'y': y,\n",
    "            'radius' : combis[i][0],\n",
    "            'length' : combis[i][1],\n",
    "            'motifs' : 2\n",
    "           }\n",
    "\n",
    "num_processors = 32                         #create a pool of processors\n",
    "p = Pool(processes = num_processors)        #get them to work in parallel#\n",
    "output = p.map(worker,[i for i in range(0,29)])\n",
    "        \n",
    "\n",
    "result_list = []\n",
    "result_list.append(output[0].keys())\n",
    "for elem in output:\n",
    "    templist = []\n",
    "    for key in elem.keys():\n",
    "        templist.append(elem[key])\n",
    "    result_list.append(templist)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('./motifs', 'wb') as motifs_file:\n",
    "    #pickle.dump(result_list[1:], motifs_file) #without the dict keys generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./motifs', 'rb') as motifs_file:\n",
    "    result_list = pickle.load(motifs_file)\n",
    "print(len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Iteration: 1-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 2-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 3-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 4-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 5-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 6-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 7-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 8-----------------\n",
      "------------------Motifs-----------------\n",
      "Motif radius: 12\n",
      "Motif length: 24\n",
      "Motif count: 2\n",
      "X shape: (3000, 2)\n",
      "y label 1: 0.5686666666666667\n",
      "y label 3: 0.43133333333333335\n",
      "\n",
      "\n",
      "------------------Sklearn-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3664).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------SVC-----------------\n",
      "{'C': 2784, 'degree': 2, 'gamma': 1000000000.0, 'kernel': 'rbf', 'max_iter': 3664, 'probability': False, 'random_state': 4, 'shrinking': False}\n",
      "0.5683333333333334\n",
      "[[325  16]\n",
      " [243  16]]\n",
      "\n",
      "\n",
      "\n",
      "------------------CART-Tree-----------------\n",
      "{'criterion': 'gini', 'max_depth': 66, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 7, 'random_state': 9, 'splitter': 'random'}\n",
      "0.6966666666666667\n",
      "[[274  67]\n",
      " [115 144]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/model_selection/_search.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Random Forrest----------------\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 4, 'n_estimators': 64, 'random_state': 2}\n",
      "0.5983333333333334\n",
      "[[301  40]\n",
      " [201  58]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------MLP----------------\n",
      "{'activation': 'tanh', 'alpha': 1e-06, 'batch_size': 4, 'early_stopping': False, 'hidden_layer_sizes': (16, 16, 16), 'learning_rate': 'adaptive', 'learning_rate_init': 10000.0, 'max_iter': 47, 'random_state': 1, 'shuffle': True, 'solver': 'lbfgs'}\n",
      "0.5966666666666667\n",
      "[[303  38]\n",
      " [204  55]]\n",
      "\n",
      "\n",
      "\n",
      "------------------Iteration: 9-----------------\n",
      "------------------Motifs-----------------\n",
      "Motif radius: 12\n",
      "Motif length: 32\n",
      "Motif count: 2\n",
      "X shape: (3000, 2)\n",
      "y label 1: 0.5516666666666666\n",
      "y label 3: 0.4483333333333333\n",
      "\n",
      "\n",
      "------------------Sklearn-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=4780).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------SVC-----------------\n",
      "{'C': 680, 'degree': 4, 'gamma': 100.0, 'kernel': 'rbf', 'max_iter': 4780, 'probability': False, 'random_state': 8, 'shrinking': False}\n",
      "0.585\n",
      "[[204 127]\n",
      " [122 147]]\n",
      "\n",
      "\n",
      "\n",
      "------------------CART-Tree-----------------\n",
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 4, 'splitter': 'random'}\n",
      "0.635\n",
      "[[245  86]\n",
      " [133 136]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/model_selection/_search.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Random Forrest----------------\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 17, 'random_state': 7}\n",
      "0.6483333333333333\n",
      "[[274  57]\n",
      " [154 115]]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------MLP----------------\n",
      "{'activation': 'logistic', 'alpha': 0.1, 'batch_size': 3, 'early_stopping': False, 'hidden_layer_sizes': (64, 64), 'learning_rate': 'invscaling', 'learning_rate_init': 1e-05, 'max_iter': 190, 'random_state': 1, 'shuffle': True, 'solver': 'lbfgs'}\n",
      "0.605\n",
      "[[281  50]\n",
      " [187  82]]\n",
      "\n",
      "\n",
      "\n",
      "------------------Iteration: 10-----------------\n",
      "Class distribution not representative\n",
      "------------------Iteration: 11-----------------\n",
      "------------------Motifs-----------------\n",
      "Motif radius: 16\n",
      "Motif length: 12\n",
      "Motif count: 2\n",
      "X shape: (3000, 2)\n",
      "y label 1: 0.524\n",
      "y label 3: 0.476\n",
      "\n",
      "\n",
      "------------------Sklearn-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/lorenz/miniconda3/envs/rctc/lib/python3.7/site-packages/sklearn/svm/base.py:241: ConvergenceWarning: Solver terminated early (max_iter=1792).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------SVC-----------------\n",
      "{'C': 409, 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': 1792, 'probability': True, 'random_state': 3, 'shrinking': True}\n",
      "0.55\n",
      "[[291  23]\n",
      " [247  39]]\n",
      "\n",
      "\n",
      "\n",
      "------------------CART-Tree-----------------\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'max_features': 1, 'min_samples_leaf': 1, 'min_samples_split': 3, 'random_state': 5, 'splitter': 'best'}\n",
      "0.6216666666666667\n",
      "[[186 128]\n",
      " [ 99 187]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for i in range(1, len(result_list)):\n",
    "    X_train = result_list[i][0][:3000]\n",
    "    y_train = result_list[i][1][:3000]\n",
    "    print(\"------------------Iteration: {}-----------------\".format(i))\n",
    "    if int(len(X)*0.2) < 100:\n",
    "        print('Test set too small')\n",
    "        continue\n",
    "    if not (0.35 < list(y_train[0]).count(1.0)/len(y_train) < 0.65):\n",
    "        print('Class distribution not representative')\n",
    "        continue\n",
    "        \n",
    "    print('------------------Motifs-----------------')\n",
    "    print(\"Motif radius: {}\".format(result_list[i][2]))\n",
    "    print(\"Motif length: {}\".format(result_list[i][3]))\n",
    "    print(\"Motif count: {}\".format(result_list[i][4]))\n",
    "    print(\"X shape: {}\".format(X_train.shape))\n",
    "    print(\"y label 1: {}\".format(list(y_train[0]).count(1.0)/len(y_train)))\n",
    "    print(\"y label 3: {}\\n\\n\".format(list(y_train[0]).count(3.0)/len(y_train)))\n",
    "    \n",
    "    #Test SVC on motif discovery\n",
    "    print('------------------Sklearn-----------------')\n",
    "    model = sklearn_factory.create_model(\n",
    "        model_type = 'svc',\n",
    "        X = X_train, \n",
    "        y = y_train, \n",
    "        model_params = {\n",
    "            'kernel': ['rbf', 'linear','poly'],\n",
    "            'degree': sp_randint(2, X_train.shape[1]*3),\n",
    "            'gamma': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "            'C': sp_randint(2, 5000),\n",
    "            'max_iter' : sp_randint(2, 5000),\n",
    "            'shrinking' : [True, False],\n",
    "            'probability' : [True, False],\n",
    "            'random_state': sp_randint(1, 10),\n",
    "        },\n",
    "        search_params = [-1, 0, 10, 100, True, \"svc_rs.pickle\", 0.2]\n",
    "        )\n",
    "    print('------------------SVC-----------------')\n",
    "    print(model['clf'].best_params_)\n",
    "    X_test, y_test = model['X_test'], model['y_test']\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    model = sklearn_factory.create_model(\n",
    "    model_type = 'cart_tree',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        \"max_depth\": sp_randint(1, 128),\n",
    "        \"max_features\": sp_randint(1, X_train.shape[1]),\n",
    "        \"min_samples_leaf\": sp_randint(1, X_train.shape[1]),\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 100, True, \"dt_rs.pickle\", 0.2]\n",
    "    )\n",
    "    print('------------------CART-Tree-----------------')\n",
    "    print(model['clf'].best_params_)\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "    model = sklearn_factory.create_model(\n",
    "        model_type = 'random_forrest',\n",
    "        X = X_train, \n",
    "        y = y_train,\n",
    "        model_params = {\n",
    "            'n_estimators' : sp_randint(1, 100),\n",
    "            'max_depth': sp_randint(1, 128),\n",
    "            #'max_features': sp_randint(1, X_train.shape[1]),\n",
    "            'min_samples_split': sp_randint(2, X_train.shape[1]),\n",
    "            'bootstrap': [True, False],\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            'random_state': sp_randint(1, 10),\n",
    "            'min_samples_split': sp_randint(2, 10)\n",
    "        },\n",
    "        search_params = [-1, 0, 10, 100, True, \"rf_rs.pickle\", 0.2]\n",
    "        )\n",
    "    print('------------------Random Forrest----------------')\n",
    "    print(model['clf'].best_params_)\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = sklearn_factory.create_model(\n",
    "    model_type = 'mlp_classifier',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'solver': ['adam', 'lbfgs', 'sgd'], \n",
    "        'max_iter': sp_randint(1, 250), \n",
    "        'alpha': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))), \n",
    "        'hidden_layer_sizes':[(128,128,128,128), #architecture see\n",
    "                              (128,128,128),\n",
    "                              (128,128),\n",
    "                              (128),\n",
    "                              (64,64,64,64),\n",
    "                              (64,64,64),\n",
    "                              (64,64),\n",
    "                              (64),\n",
    "                              (32,32,32,32),\n",
    "                              (32,32,32),\n",
    "                              (32,32),\n",
    "                              (32),\n",
    "                              (16,16,16,16),\n",
    "                              (16,16,16),\n",
    "                              (16,16),\n",
    "                              (16)\n",
    "                             ], \n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'activation': [\"logistic\", \"relu\", \"tanh\"],\n",
    "        'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'learning_rate_init' : np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'batch_size' : sp_randint(1, 10),\n",
    "        'shuffle' :[True, False],\n",
    "        'early_stopping' : [True, False],\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 25, True, \"mlp_rs.pickle\", 0.2]\n",
    "    )\n",
    "    print('------------------MLP----------------')\n",
    "    print(model['clf'].best_params_)\n",
    "    print(model['clf'].score(X_test, y_test))\n",
    "    y_pred = model['clf'].predict(X_test)\n",
    "    conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "    print(conf)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Store in correct format for Sequitur motif discovery\n",
    "motif_data=data.reindex(columns=['road_label', \n",
    "                                 #'acceleration_abs',\n",
    "                                 #'acceleration_abs_2'\n",
    "                                 #'acceleration_x', 'acceleration_y', 'acceleration_z',\n",
    "                                 #'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                                 #'gravity_x', 'gravity_y', 'gravity_z',\n",
    "                                 #'orientation_x', 'orientation_y', 'orientation_z', 'orientation_w',\n",
    "                                 'l_acceleration_x', 'l_acceleration_y', 'l_acceleration_z',\n",
    "                    \n",
    "                                ]).astype('float') #'acceleration_abs'\n",
    "motif_data['road_label']=motif_data['road_label'].astype('int') \n",
    "\n",
    "motif_data = preprocessor.remove_nans(motif_data, replacement_mode='del_row')\n",
    "test_sz = 0.1\n",
    "#print(motif_data)\n",
    "print(list(motif_data['road_label']).count(1.0)/len(motif_data))\n",
    "print(list(motif_data['road_label']).count(3.0)/len(motif_data))\n",
    "\n",
    "motif_data_test = motif_data.head(int(len(motif_data)*test_sz))\n",
    "motif_data_train = motif_data.tail(len(motif_data)-int(len(motif_data)*test_sz))\n",
    "print(motif_data_test.shape)\n",
    "print(motif_data_train.shape)\n",
    "print(list(motif_data_train['road_label']).count(1.0)/len(motif_data_train))\n",
    "print(list(motif_data_train['road_label']).count(3.0)/len(motif_data_train))\n",
    "print(list(motif_data_test['road_label']).count(1.0)/len(motif_data_test))\n",
    "print(list(motif_data_test['road_label']).count(3.0)/len(motif_data_test))\n",
    "\n",
    "#np.savetxt(r'./motif_data_test.txt', motif_data_test.values, fmt='%1.6f')\n",
    "#np.savetxt(r'./motif_data_train.txt', motif_data_train.values, fmt='%1.6f')\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "#print(motif_data_test)\n",
    "\n",
    "#motif_data_train_small = motif_data.tail(int(len(motif_data)*0.5)-int(len(motif_data)*test_sz*0.5))\n",
    "#motif_data_test_small = motif_data.head(int(len(motif_data)*test_sz*0.5))\n",
    "#print(motif_data_test_small.shape)\n",
    "#print(motif_data_train_small.shape)\n",
    "#print(list(motif_data_train_small['road_label']).count(1.0)/len(motif_data_train_small))\n",
    "#print(list(motif_data_train_small['road_label']).count(3.0)/len(motif_data_train_small))\n",
    "#print(list(motif_data_test_small['road_label']).count(1.0)/len(motif_data_test_small))\n",
    "#print(list(motif_data_test_small['road_label']).count(3.0)/len(motif_data_test_small))\n",
    "\n",
    "#print(motif_data_test_small)\n",
    "#np.savetxt(r'./motif_data_test_small.txt', motif_data_test_small.values, fmt='%1.4f')\n",
    "#np.savetxt(r'./motif_data_train_small.txt', motif_data_train_small.values, fmt='%1.4f')\n",
    "#np.savetxt(r'./X_Train.txt', motif_data_train['acceleration_abs'].values, fmt='%1.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visual anlaysis of the segments:\n",
    "#sns.set(rc={'figure.figsize':(15, 4)})\n",
    "#fig, ax = plt.subplots(figsize=(15,4*len(data_segments)), ncols=1, nrows=len(data_segments)+1)\n",
    "#for ind in range(len(data_segments)): \n",
    "#    sns.lineplot(y='acceleration_abs', x='time', data = data_segments[ind], ax=ax[ind])\n",
    "#    ax[ind].legend(\"Road\" if data_segments[ind]['road_label'].iloc[0] < 2.0 else \"City\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16, 6))\n",
    "#sns.lineplot(data=motif_data_train[['acceleration_abs', 'road_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16, 6))\n",
    "#sns.lineplot(data=motif_data_train[['l_acceleration_y', 'road_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extraction\n",
    "# 3.1 Encode categorical to binary\n",
    "data = preprocessor.encode_categorical_features(data = data, \n",
    "                                                mode = 'custom_function', \n",
    "                                                columns = ['road_label'],\n",
    "                                                encoding_function = lambda x :  (x  > 2.0).astype(int)\n",
    "                                               ) #0 City, 1 Countryside\n",
    "\n",
    "# 3.2\n",
    "# Generate label vector y and feature matrix X.\n",
    "# We need at least 2 classes to learn features for tsfresh\n",
    "y = data[['road_label']].reset_index(drop=True)\n",
    "data['id'] = range(1, len(data) + 1)\n",
    "y['id'] = data['id']\n",
    "y['road_label'].index=list(y['id'])\n",
    "\n",
    "# 3.3 Extract feature matrix\n",
    "# Read https://github.com/blue-yonder/tsfresh/issues/444 for info about the warnings\n",
    "#if task is just inference, use extractor and select features found relevant during training.\n",
    "X = extractor.extract_features(data = data, args = ['id', 32, None]) \n",
    "X = extractor.select_features(data = X, args = [y['road_label'], 32, None, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3.3.1 Read/Write extracted features\n",
    "#dao.write_features('./data_sets/X.pkl', X)\n",
    "#dao.write_features('./data_sets/y.pkl', y)\n",
    "X = dao.load_features('./data_sets/X.pkl')\n",
    "y = dao.load_features('./data_sets/y.pkl')\n",
    "print(len(y))\n",
    "keys = X.keys()\n",
    "keys = list(filter(lambda x: \"acceleration_abs\" in x, keys))\n",
    "print(X.shape)\n",
    "#print(y)\n",
    "print(list(y['road_label']).count(0)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 combine feature rows\n",
    "X_join = pandas.concat([X, y], axis=1)\n",
    "X_join = preprocessor.remove_nans(X_join, replacement_mode='del_row')\n",
    "X_join[['road_label']] = X_join[['road_label']].astype('int')\n",
    "X_segments = preprocessor.segment_data(X_join, mode='labels', \n",
    "                                    label_column='road_label', \n",
    "                                    args=[0,1])\n",
    "\n",
    "\n",
    "segment_length = 30 #60s best in paper, 90 best in my evaluation, tested 30, 60, 90, 120\n",
    "X_segments_new = []\n",
    "for ind in range(0, len(X_segments)):\n",
    "    X_segments_new = X_segments_new + preprocessor.segment_data(\n",
    "        X_segments[ind],\n",
    "        mode = 'fixed_interval', \n",
    "        args = [segment_length, True, True]\n",
    "    )\n",
    "    \n",
    "    \n",
    "print(len(X_segments_new))\n",
    "keys.append('road_label')\n",
    "X_combined = preprocessor.de_segment_data(X_segments_new, keys)\n",
    "X_combined, y_combined = X_combined[keys[:-1]], X_combined[keys[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#X_combined.hist(figsize=(15,15)) #check ditrsibution -> normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.matshow(X_combined.corr(), fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,10))\n",
    "#plt.matshow(X_combined.cov(), fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 3.5 Read/Write combined features\n",
    "#print(type(y_combined))\n",
    "dao.write_features('./data_sets/X_combined.pkl', X_combined)\n",
    "dao.write_features('./data_sets/y_combined.pkl', y_combined)\n",
    "# 3.6  extracted features\n",
    "X_combined = dao.load_features('./data_sets/X_combined.pkl')\n",
    "y_combined = dao.load_features('./data_sets/y_combined.pkl')\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(list(y_combined).count(0)/len(y_combined))\n",
    "  \n",
    "y_clustering = IsolationForest(behaviour='new', \n",
    "                               max_samples=5, \n",
    "                               n_jobs=-1, \n",
    "                               contamination=0.25,\n",
    "                               max_features=1.0,\n",
    "                               n_estimators=750\n",
    "                              ).fit_predict(X_combined)\n",
    "\n",
    "X_combined = X_combined.loc[pandas.DataFrame(y_clustering)[0] == 1]\n",
    "y_combined = y_combined.loc[pandas.DataFrame(y_clustering)[0] == 1]\n",
    "\n",
    "print(X_combined.shape)\n",
    "print(list(y_combined).count(0)/len(y_combined))\n",
    "\n",
    "X_combined = X_combined.reset_index(drop=True)\n",
    "y_combined = y_combined.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#X_combined.hist(figsize=(15,15)) #check ditrsibution -> normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried but failed improvbement methods\n",
    "#1\n",
    "#https://en.wikipedia.org/wiki/Feature_selection#Correlation_feature_selection\n",
    "#X_combined = X_combined[X_combined.columns[[0,1,3,20,21,22]]\n",
    "# Didnt help\n",
    "\n",
    "# Preclustering using DBSCAN, Optics and KMeans (last on normalized Dataset) and adding prediciton \n",
    "# as new feature made results worse.\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_combined,\n",
    "                                                    y_combined,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=y_combined\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4.1 Produce models from a given hyper parameter search space\n",
    "#TODO: Put model types and parametzer and search spaces into config.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('------------------Sklearn-----------------')\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'svc',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'kernel': ['rbf', 'linear','poly'],\n",
    "        'degree': sp_randint(2, X_combined.shape[1]*3),\n",
    "        'gamma': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'C': sp_randint(2, 5000),\n",
    "        'max_iter' : sp_randint(2, 5000),\n",
    "        'shrinking' : [True, False],\n",
    "        'probability' : [True, False],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"svc_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------SVC-----------------')\n",
    "X_test, y_test = model['X_test'], model['y_test']\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'cart_tree',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        \"max_depth\": sp_randint(1, 128),\n",
    "        \"max_features\": sp_randint(1, X_combined.shape[1]),\n",
    "        \"min_samples_leaf\": sp_randint(1, X_combined.shape[1]),\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'splitter' : ['best', 'random'],\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"dt_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------CART-Tree-----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'random_forrest',\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    model_params = {\n",
    "        'n_estimators' : sp_randint(1, 100),\n",
    "        'max_depth': sp_randint(1, 128),\n",
    "        'max_features': sp_randint(1, X_combined.shape[1]),\n",
    "        'min_samples_split': sp_randint(2, X_combined.shape[1]),\n",
    "        'bootstrap': [True, False],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'min_samples_split': sp_randint(2, 10)\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 2500, True, \"rf_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------Random Forrest----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = sklearn_factory.create_model(\n",
    "    model_type = 'mlp_classifier',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'solver': ['adam', 'lbfgs', 'sgd'], \n",
    "        'max_iter': sp_randint(1, 250), \n",
    "        'alpha': np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))), \n",
    "        'hidden_layer_sizes':[(128,128,128,128), #architecture see\n",
    "                              (128,128,128),\n",
    "                              (128,128),\n",
    "                              (128),\n",
    "                              (64,64,64,64),\n",
    "                              (64,64,64),\n",
    "                              (64,64),\n",
    "                              (64),\n",
    "                              (32,32,32,32),\n",
    "                              (32,32,32),\n",
    "                              (32,32),\n",
    "                              (32),\n",
    "                              (16,16,16,16),\n",
    "                              (16,16,16),\n",
    "                              (16,16),\n",
    "                              (16)\n",
    "                             ], \n",
    "        'random_state': sp_randint(1, 10),\n",
    "        'activation': [\"logistic\", \"relu\", \"tanh\"],\n",
    "        'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'learning_rate_init' : np.concatenate((10.0 ** -np.arange(0, 10),10.0 ** np.arange(1, 10))),\n",
    "        'batch_size' : sp_randint(1, 10),\n",
    "        'shuffle' :[True, False],\n",
    "        'early_stopping' : [True, False],\n",
    "    },\n",
    "    search_params = [-1, 0, 10, 100, True, \"mlp_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------MLP----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "model = tslearn_factory.create_model(\n",
    "    model_type = 'tssvc',\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    model_params = {\n",
    "        'kernel': ['rbf', 'linear','poly', 'gak'],\n",
    "        'degree': sp_randint(2, X_combined.shape[1]*3),\n",
    "        'gamma': np.concatenate((10.0 ** -np.arange(0, 5),10.0 ** np.arange(1, 10))),\n",
    "        'max_iter' : sp_randint(2, 5000),\n",
    "        'shrinking' : [True, False],\n",
    "        'probability' : [True, False],\n",
    "        'random_state': sp_randint(1, 10),\n",
    "    },\n",
    "    search_params = [32, 0, 10, 250, True, \"tssv_rs.pickle\", 0.2]\n",
    "    )\n",
    "\n",
    "print('------------------Tslearn-----------------')\n",
    "print('------------------TSSVC----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "model = tslearn_factory.create_model(\n",
    "    model_type = 'knn_classifier',\n",
    "    X = X_train, \n",
    "    y = y_train, \n",
    "    model_params = {\n",
    "        'n_neighbors' : sp_randint(2, X_combined.shape[1]*2),\n",
    "        'metric' : ['dtw', 'softdtw', 'euclidean', 'sqeuclidean', 'cityblock']\n",
    "    },\n",
    "    search_params = [32, 0, 10, 250, True, \"tsknn_rs.pickle\", 0.2]\n",
    "    )\n",
    "print('------------------KNNC----------------')\n",
    "print(model['clf'].score(X_test, y_test))\n",
    "y_pred = model['clf'].predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)\n",
    "print(conf)\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Evaluate best model using validation set\n",
    "print('Testset:')\n",
    "print('Shape: '+str(X_test.shape))\n",
    "print('% class 0 (City)'+str(list(y_test).count(0)/len(y_test)))\n",
    "print('\\n')\n",
    "print('Validationset:')\n",
    "print('Shape: '+str(X_validation.shape))\n",
    "print('% class 0 (City)'+str(list(y_validation).count(0)/len(y_validation)))\n",
    "print('\\n')\n",
    "\n",
    "print(\"----------------sklearn----------------\")\n",
    "print(\"MLP\")\n",
    "with open('mlp_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"CART Tree\")\n",
    "with open('dt_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Random Forrest\")\n",
    "with open('rf_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"SVC\")\n",
    "with open('svc_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"----------------tslearn----------------\")\n",
    "print(\"TSSVC\")\n",
    "with open('tssv_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"TSKNN\")\n",
    "with open('tsknn_rs.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print('Score on Testset: '+str(clf.score(X_test, y_test)))\n",
    "print('Report for Validationset:')\n",
    "print(str(classification_report(y_validation, clf.predict(X_validation))))\n",
    "print(clf.best_params_)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
